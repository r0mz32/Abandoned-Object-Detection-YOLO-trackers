{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZz3Gp7m4UnW",
        "outputId": "17f7d691-1769-43a2-a656-fb5414db4bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 20 17:21:00 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependences"
      ],
      "metadata": {
        "id": "nT8rQsnpAci_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6mdIlMXAb5T",
        "outputId": "803b780a-af27-43c4-8531-ed9fa0f4c9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from typing import Generator, Tuple, Optional, List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "RftnLV7AEC0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Detectors & Trackers"
      ],
      "metadata": {
        "id": "ulru5wtwBd57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###YOLOv5"
      ],
      "metadata": {
        "id": "ZTTevDTzjPT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt\n",
        "import utils\n",
        "display = utils.notebook_init()\n",
        "%cd {HOME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uNqwTJQVjO2c",
        "outputId": "38a6a41a-8fad-4749-af9b-f078e546444b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-243-g7c54e5d Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 26.9/78.2 GB disk)\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###YOLOv8"
      ],
      "metadata": {
        "id": "fCe-8bbckLck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqNhtgzAjNDk",
        "outputId": "46ab3e77-a08e-4c9e-843c-6e8d8c7c4c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supervision\n",
            "  Downloading supervision-0.16.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.2/72.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from supervision) (4.8.1.78)\n",
            "Requirement already satisfied: pillow<11.0,>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision) (10.1.0)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (6.0.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.11.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->supervision) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.1->supervision) (1.16.0)\n",
            "Installing collected packages: supervision\n",
            "Successfully installed supervision-0.16.0\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.0.215)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ByteTrack"
      ],
      "metadata": {
        "id": "ZFBFex3SjWMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "%cd {HOME}/ByteTrack\n",
        "\n",
        "!pip3 install -q -r requirements.txt\n",
        "!python3 setup.py -q develop\n",
        "!pip install -q cython_bbox\n",
        "!pip install -q onemetric\n",
        "!pip install -q loguru lap thop\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(f\"{HOME}/ByteTrack\")\n",
        "\n",
        "\n",
        "import yolox\n",
        "print(\"yolox.__version__:\", yolox.__version__)\n",
        "%cd {HOME}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPtnm0K8jUH-",
        "outputId": "0e910e14-db70-4299-9763-f23a7a820e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolox.__version__: 0.1.0\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###StrongSORT"
      ],
      "metadata": {
        "id": "waYn2q-Tjca0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/mikel-brostrom/yolo_tracking.git\n",
        "%cd {HOME}/yolo_tracking\n",
        "\n",
        "!pip3 install -q -r requirements.txt\n",
        "!python3 setup.py -q develop\n",
        "\n",
        "sys.path.append(f\"{HOME}/yolo_tracking\")\n",
        "%cd {HOME}"
      ],
      "metadata": {
        "id": "cMZvHNeOjmeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SORT"
      ],
      "metadata": {
        "id": "6ovlpTnkjqAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libopencv-dev\n",
        "!apt install libeigen3-dev\n",
        "!pip install sort-tracker\n",
        "import sort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwuu1sWrj3mC",
        "outputId": "d3a04d2f-02ca-4c5c-eaec-653ae3e36141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopencv-dev is already the newest version (4.5.4+dfsg-9ubuntu4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libeigen3-doc libmpfrc++-dev\n",
            "The following NEW packages will be installed:\n",
            "  libeigen3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 9 not upgraded.\n",
            "Need to get 1,056 kB of archives.\n",
            "After this operation, 9,081 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeigen3-dev all 3.4.0-2ubuntu2 [1,056 kB]\n",
            "Fetched 1,056 kB in 1s (1,071 kB/s)\n",
            "Selecting previously unselected package libeigen3-dev.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../libeigen3-dev_3.4.0-2ubuntu2_all.deb ...\n",
            "Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Collecting sort-tracker\n",
            "  Downloading sort-tracker-1.0.7.tar.gz (27 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sort-tracker) (1.23.5)\n",
            "Building wheels for collected packages: sort-tracker\n",
            "  Building wheel for sort-tracker (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sort-tracker: filename=sort_tracker-1.0.7-cp310-cp310-linux_x86_64.whl size=108917 sha256=c7c3e8cec3f02ca84b89350572f0489460b8ffd14ae7dc16beaf5969c4ee9da8\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/33/4f/648a583e11eeaf6f96dfd55b58c5a940ec2d61893994185677\n",
            "Successfully built sort-tracker\n",
            "Installing collected packages: sort-tracker\n",
            "Successfully installed sort-tracker-1.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Weifhts & Videos"
      ],
      "metadata": {
        "id": "IYm6qYBuk3c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1JJQb8IGv5NrhRiTFabrjFULwSrDMbmMs\n",
        "!unzip TEST.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHSvFBU_7b99",
        "outputId": "d97782d6-1e90-4c6f-d255-22d5d48b6d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JJQb8IGv5NrhRiTFabrjFULwSrDMbmMs\n",
            "To: /content/TEST.zip\n",
            "100% 1.52G/1.52G [00:16<00:00, 90.2MB/s]\n",
            "Archive:  TEST.zip\n",
            "   creating: TEST/\n",
            "   creating: TEST/test1/\n",
            "  inflating: TEST/test1/video.avi    \n",
            "  inflating: TEST/test1/YOLOv5.pt    \n",
            "  inflating: TEST/test1/YOLOv8.pt    \n",
            "  inflating: TEST/test1/labels.txt   \n",
            "  inflating: TEST/test1/background.png  \n",
            "   creating: TEST/test2/\n",
            "  inflating: TEST/test2/video.avi    \n",
            "  inflating: TEST/test2/YOLOv5.pt    \n",
            "  inflating: TEST/test2/YOLOv8.pt    \n",
            "  inflating: TEST/test2/labels.txt   \n",
            "  inflating: TEST/test2/background.png  \n",
            "   creating: TEST/test3/\n",
            "  inflating: TEST/test3/labels.txt   \n",
            "  inflating: TEST/test3/video.avi    \n",
            "  inflating: TEST/test3/YOLOv5.pt    \n",
            "  inflating: TEST/test3/YOLOv8.pt    \n",
            "  inflating: TEST/test3/background.png  \n",
            "   creating: TEST/test4/\n",
            "  inflating: TEST/test4/labels.txt   \n",
            "  inflating: TEST/test4/video.avi    \n",
            "  inflating: TEST/test4/YOLOv5.pt    \n",
            "  inflating: TEST/test4/YOLOv8.pt    \n",
            "  inflating: TEST/test4/background.jpg  \n",
            "   creating: TEST/test5/\n",
            "  inflating: TEST/test5/labels.txt   \n",
            "  inflating: TEST/test5/video.avi    \n",
            "  inflating: TEST/test5/YOLOv5.pt    \n",
            "  inflating: TEST/test5/YOLOv8.pt    \n",
            "  inflating: TEST/test5/background.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Config"
      ],
      "metadata": {
        "id": "9VLGAq7YCjQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Base Classes"
      ],
      "metadata": {
        "id": "uC20bZ5EC1wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geometry utilities"
      ],
      "metadata": {
        "id": "6a96rhcUVlzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "    @property\n",
        "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
        "        return int(self.x), int(self.y)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Rect:\n",
        "    x: float\n",
        "    y: float\n",
        "    width: float\n",
        "    height: float\n",
        "\n",
        "    @property\n",
        "    def min_x(self) -> float:\n",
        "        return self.x\n",
        "\n",
        "    @property\n",
        "    def min_y(self) -> float:\n",
        "        return self.y\n",
        "\n",
        "    @property\n",
        "    def s_width(self) -> float:\n",
        "        return self.x + self.width\n",
        "\n",
        "    @property\n",
        "    def s_height(self) -> float:\n",
        "        return self.y + self.height\n",
        "\n",
        "    @property\n",
        "    def max_x(self) -> float:\n",
        "        return self.x + self.width\n",
        "\n",
        "    @property\n",
        "    def max_y(self) -> float:\n",
        "        return self.y + self.height\n",
        "\n",
        "    @property\n",
        "    def top_left(self) -> Point:\n",
        "        return Point(x=self.x, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def bottom_right(self) -> Point:\n",
        "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def bottom_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def top_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height / 2)\n",
        "\n",
        "    def get_bottom_center(self) -> tuple:\n",
        "      return (int(self.x + self.width / 2), int(self.y + self.height))\n",
        "\n",
        "    def get_center(self) -> tuple:\n",
        "      return (int(self.x + self.width / 2), int(self.y + self.height / 2))\n",
        "\n",
        "    def min_max_cords(self):\n",
        "\n",
        "      return self.min_x, self.min_y, self.max_x, self.max_y\n",
        "\n",
        "\n",
        "    def pad(self, padding: float) -> Rect:\n",
        "        return Rect(\n",
        "            x=self.x - padding,\n",
        "            y=self.y - padding,\n",
        "            width=self.width + 2*padding,\n",
        "            height=self.height + 2*padding\n",
        "        )\n",
        "\n",
        "\n",
        "    def contains_point(self, point: Point) -> bool:\n",
        "        return self.min_x < point.x < self.max_x and self.min_y < point.y < self.max_y"
      ],
      "metadata": {
        "id": "b2zRdNIEQNjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detection utilities"
      ],
      "metadata": {
        "id": "alKpUzPtVwFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Detection:\n",
        "    rect: Rect\n",
        "    class_id: int\n",
        "    class_name: str\n",
        "    confidence: float\n",
        "    tracker_id: Optional[int] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_results(cls, pred: np.ndarray, names: Dict[int, str]) -> List[Detection]:\n",
        "        result = []\n",
        "        for x_min, y_min, x_max, y_max, confidence, class_id in pred:\n",
        "            class_id=int(class_id)\n",
        "            result.append(Detection(\n",
        "                rect=Rect(\n",
        "                    x=float(x_min),\n",
        "                    y=float(y_min),\n",
        "                    width=float(x_max - x_min),\n",
        "                    height=float(y_max - y_min)\n",
        "                ),\n",
        "                class_id=class_id,\n",
        "                class_name=names[class_id],\n",
        "                confidence=float(confidence)\n",
        "            ))\n",
        "        return result\n",
        "\n",
        "    def available_for_annotation(self):\n",
        "      if self.class_id != None and self.class_name != None and self.tracker_id != None:\n",
        "        print(self.tracker_id)\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "def filter_detections_by_class(detections: List[Detection], class_name: str) -> List[Detection]:\n",
        "    return [\n",
        "        detection\n",
        "        for detection\n",
        "        in detections\n",
        "        if detection.class_name == class_name\n",
        "    ]"
      ],
      "metadata": {
        "id": "1Yvp1WfdVuUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Draw utilities"
      ],
      "metadata": {
        "id": "jOim2wDQV52_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class Color:\n",
        "    r: int\n",
        "    g: int\n",
        "    b: int\n",
        "\n",
        "    @property\n",
        "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
        "        return self.b, self.g, self.r\n",
        "\n",
        "    @classmethod\n",
        "    def from_hex_string(cls, hex_string: str) -> Color:\n",
        "        r, g, b = tuple(int(hex_string[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "        return Color(r=r, g=g, b=b)\n",
        "\n",
        "\n",
        "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_filled_rect(image: np.ndarray, rect: Rect, color: Color) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_polygon(image: np.ndarray, countour: np.ndarray, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.drawContours(image, [countour], 0, color.bgr_tuple, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_filled_polygon(image: np.ndarray, countour: np.ndarray, color: Color) -> np.ndarray:\n",
        "    cv2.drawContours(image, [countour], 0, color.bgr_tuple, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_text(image: np.ndarray, anchor: Point, text: str, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.putText(image, text, anchor.int_xy_tuple, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color.bgr_tuple, thickness, 2, False)\n",
        "    return image"
      ],
      "metadata": {
        "id": "0N218wSkV6S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annotation utilities"
      ],
      "metadata": {
        "id": "krldTNOsWnsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BaseAnnotator:\n",
        "    colors: List[Color]\n",
        "    thickness: int\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            annotated_image = draw_rect(\n",
        "                image=image,\n",
        "                rect=detection.rect,\n",
        "                color=self.colors[detection.class_id],\n",
        "                thickness=self.thickness\n",
        "            )\n",
        "        return annotated_image\n",
        "\n",
        "# calculates coordinates of possession marker\n",
        "def calculate_marker(anchor: Point) -> np.ndarray:\n",
        "    x, y = anchor.int_xy_tuple\n",
        "    return(np.array([\n",
        "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
        "        [x, y - MARKER_MARGIN],\n",
        "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
        "    ]))\n",
        "\n",
        "\n",
        "# draw single possession marker\n",
        "def draw_marker(image: np.ndarray, anchor: Point, color: Color) -> np.ndarray:\n",
        "    possession_marker_countour = calculate_marker(anchor=anchor)\n",
        "    image = draw_filled_polygon(\n",
        "        image=image,\n",
        "        countour=possession_marker_countour,\n",
        "        color=color)\n",
        "    image = draw_polygon(\n",
        "        image=image,\n",
        "        countour=possession_marker_countour,\n",
        "        color=MARKER_CONTOUR_COLOR,\n",
        "        thickness=MARKER_CONTOUR_THICKNESS)\n",
        "    return image\n",
        "\n",
        "\n",
        "# dedicated annotator to draw possession markers on video frames\n",
        "@dataclass\n",
        "class MarkerAnntator:\n",
        "\n",
        "    color: Color\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            annotated_image = draw_marker(\n",
        "                image=image,\n",
        "                anchor=detection.rect.top_center,\n",
        "                color=self.color)\n",
        "        return annotated_image\n",
        "\n",
        "@dataclass\n",
        "class ObjectAnnotator:\n",
        "\n",
        "    color: Color\n",
        "\n",
        "    def circle(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            annotated_image = cv2.circle(annotated_image, detection.rect.get_bottom_center(), 2*int(detection.rect.width), self.color.bgr_tuple, 2)\n",
        "        return annotated_image\n",
        "\n",
        "    def line(self, image: np.ndarray, object_detections: List[Detection], person_detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for person_detection in person_detections:\n",
        "            person_center= person_detection.rect.get_center()\n",
        "            for object_detection in object_detections:\n",
        "                object_center = object_detection.rect.get_center()\n",
        "                annotated_image = cv2.line(annotated_image,person_center,object_center,self.color.bgr_tuple,2)\n",
        "        return annotated_image\n",
        "\n",
        "# stores information about output video file, width and height of the frame must be equal to input video\n",
        "@dataclass(frozen=True)\n",
        "class VideoConfig:\n",
        "    fps: float\n",
        "    width: int\n",
        "    height: int\n",
        "\n",
        "\n",
        "# create cv2.VideoWriter object that we can use to save output video\n",
        "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
        "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
        "    os.makedirs(video_target_dir, exist_ok=True)\n",
        "    return cv2.VideoWriter(\n",
        "        target_video_path,\n",
        "        fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps=video_config.fps,\n",
        "        frameSize=(video_config.width, video_config.height),\n",
        "        isColor=True\n",
        "    )\n",
        "\n",
        "# text annotator to display tracker_id\n",
        "@dataclass\n",
        "class TextAnnotator:\n",
        "    background_color: Color\n",
        "    text_color: Color\n",
        "    text_thickness: int\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            # if tracker_id is not assigned skip annotation\n",
        "            if detection.tracker_id is None:\n",
        "                continue\n",
        "\n",
        "            # calculate text dimensions\n",
        "            size, _ = cv2.getTextSize(\n",
        "                str(detection.tracker_id),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7,\n",
        "                thickness=self.text_thickness)\n",
        "            width, height = size\n",
        "\n",
        "            # calculate text background position\n",
        "            center_x, center_y = detection.rect.bottom_center.int_xy_tuple\n",
        "            x = center_x - width // 2\n",
        "            y = center_y - height // 2 + 10\n",
        "\n",
        "            # draw background\n",
        "            annotated_image = draw_filled_rect(\n",
        "                image=annotated_image,\n",
        "                rect=Rect(x=x, y=y, width=width, height=height).pad(padding=5),\n",
        "                color=self.background_color)\n",
        "\n",
        "            # draw text\n",
        "            annotated_image = draw_text(\n",
        "                image=annotated_image,\n",
        "                anchor=Point(x=x, y=y + height),\n",
        "                text=str(detection.tracker_id),\n",
        "                color=self.text_color,\n",
        "                thickness=self.text_thickness)\n",
        "        return annotated_image"
      ],
      "metadata": {
        "id": "CVLi7DXbWoDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_frames(video_file: str) -> Generator[np.ndarray, None, None]:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        yield frame\n",
        "\n",
        "    video.release()\n",
        "\n",
        "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
        "    plt.figure(figsize=(size, size))\n",
        "    plt.imshow(image[...,::-1])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "e1racKKzB6C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular objects detection methods"
      ],
      "metadata": {
        "id": "kXuNags78mO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ellipse_check(center,point,rad,coef):\n",
        "    first= (coef[0]*(point[0]-center[0])**2 + coef[1]*((point[0]-center[0])*(point[1]-center[1])) + coef[2]*(point[1]-center[1])**2)\n",
        "    rad=rad**2\n",
        "    if( first <= rad):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def ellipse_check2(center,point,rad,coef):\n",
        "    first= (coef[0]*(point[0]-center[0])**2 + coef[1]*((point[0]-center[0])*(point[1]-center[1])) + coef[2]*(point[1]-center[1])**2)\n",
        "    rad=rad**2\n",
        "    if( first >= (0.9 * rad) and first <= (1.1*rad)):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def draw_the_ellipse(center, image,radius, coef):\n",
        "    h, w, c= image.shape\n",
        "    image = cv2.circle(image, center, radius=1, color=(99, 112, 222), thickness=-1)\n",
        "    for x in range(w):\n",
        "        for y in range(h):\n",
        "            if(x % 4 == 0 and y % 4 == 0 and ellipse_check2(center, (x,y), radius,coef)) :\n",
        "                image = cv2.circle(image, (x, y), radius=2, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "    return image\n",
        "\n",
        "def draw_elipce_around_abandoned_object(\n",
        "    object_detections: List[Detection],\n",
        "    image, ellipse_ratio\n",
        "):\n",
        "  for object_detection in object_detections:\n",
        "\n",
        "    object_coordinates = object_detection.rect.get_bottom_center()\n",
        "    object_width= int(object_detection.rect.width)\n",
        "\n",
        "    image = draw_the_ellipse(object_coordinates, image, 35*object_width, ellipse_ratio)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "K71NSf_Y8sFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Point in a circle\n",
        "\n",
        "A method of searching for regular objects based on finding at least one point of a person's box inside a circle\n"
      ],
      "metadata": {
        "id": "EiFq5xGjYi8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_regular_object(\n",
        "    person_detections: List[Detection],\n",
        "    object_detections: List[Detection]\n",
        ") -> Optional[Detection]:\n",
        "\n",
        "  regular=[]\n",
        "  for object_detection in object_detections:\n",
        "    for person_detection in person_detections:\n",
        "      if object_detection.rect.pad(2*int(object_detection.rect.width)).contains_point(point=person_detection.rect.bottom_center):\n",
        "        regular.append(object_detection)\n",
        "  return regular"
      ],
      "metadata": {
        "id": "uCWs2aUH8wrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_regular_object(\n",
        "#     person_detections: List[Detection],\n",
        "#     object_detections: List[Detection],\n",
        "#     image\n",
        "# ) -> Optional[Detection]:\n",
        "\n",
        "#   regular=[]\n",
        "#   for object_detection in object_detections:\n",
        "#     for person_detection in person_detections:\n",
        "#       if object_detection.rect.pad(2*int(object_detection.rect.width)).contains_point(point=person_detection.rect.bottom_center):\n",
        "#         person_center= person_detection.rect.get_center()\n",
        "#         print(object_detection.rect.get_bottom_center())\n",
        "#         regular.append(object_detection)\n",
        "#         image = cv2.circle(image, object_detection.rect.get_bottom_center(), 2*int(object_detection.rect.width), (0,255,0), 2)\n",
        "#       else:\n",
        "#         print(object_detection.rect.get_bottom_center())\n",
        "#         image = cv2.circle(image, object_detection.rect.get_bottom_center(), 2*int(object_detection.rect.width), (0,0,255), 2)\n",
        "#   return regular, image"
      ],
      "metadata": {
        "id": "d6uvmOi91Pu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A point on an ellipse\n",
        "\n",
        "A method of searching for ordinary objects based on finding the middle of the lower face of a person's box inside an ellipse, the size of which depends on the width of the box of the object"
      ],
      "metadata": {
        "id": "crIFLE3fZMbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_regular_object_second(\n",
        "    person_detections: List[Detection],\n",
        "    object_detections: List[Detection],\n",
        "    ellipse_ratio\n",
        ") -> Optional[Detection]:\n",
        "  regular=[]\n",
        "  for object_detection in object_detections:\n",
        "    for person_detection in person_detections:\n",
        "\n",
        "      object_coordinates = object_detection.rect.get_bottom_center()\n",
        "      person_coordinates = person_detection.rect.get_bottom_center()\n",
        "      object_width= int(object_detection.rect.width)\n",
        "\n",
        "      if (ellipse_check(object_coordinates, person_coordinates, 50*object_width, ellipse_ratio)):\n",
        "        regular.append(object_detection)\n",
        "\n",
        "  return regular"
      ],
      "metadata": {
        "id": "mtyV7Xw_80SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A point in an ellipse\n",
        "\n",
        "A method of searching for regular objects based on finding at least one point of a person's box inside an ellipse, the size of which\n",
        "depends on the width of the box of the object"
      ],
      "metadata": {
        "id": "b_0XHgqfaFFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_regular_object_third(\n",
        "    person_detections: List[Detection],\n",
        "    object_detections: List[Detection],\n",
        "    ellipse_ratio\n",
        ") -> Optional[Detection]:\n",
        "  regular=[]\n",
        "  for object_detection in object_detections:\n",
        "    for person_detection in person_detections:\n",
        "      object_coordinates = object_detection.rect.get_bottom_center()\n",
        "      object_width= int(object_detection.rect.width)\n",
        "      min_x, min_y, max_x, max_y= person_detection.rect.min_max_cords()\n",
        "      loop_breaker=0\n",
        "\n",
        "      for y in range(int(min_y), int(max_y)):\n",
        "        for x in range(int(min_x), int(max_x)):\n",
        "          if (ellipse_check(object_coordinates, (x,y), 35*object_width, ellipse_ratio)):\n",
        "            regular.append(object_detection)\n",
        "            loop_breaker=1\n",
        "            break\n",
        "        if loop_breaker: break\n",
        "  return regular"
      ],
      "metadata": {
        "id": "xGG4-YXONx37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Euclidean distance\n",
        "\n",
        "The method of searching for regular objects based on the Euclidean distance between the centers of the boxes\n",
        "of a person and an object, the distance depends on the width of the box of the object."
      ],
      "metadata": {
        "id": "F_jhTHw9a5zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def get_regular_object_fourth(\n",
        "    person_detections: List[Detection],\n",
        "    object_detections: List[Detection]\n",
        ") -> Optional[Detection]:\n",
        "  regular=[]\n",
        "  for object_detection in object_detections:\n",
        "    for person_detection in person_detections:\n",
        "\n",
        "      object_center = object_detection.rect.get_center()\n",
        "      person_center= person_detection.rect.get_center()\n",
        "\n",
        "      object_width= int(object_detection.rect.width)\n",
        "      dst = distance.euclidean(object_center, person_center)\n",
        "\n",
        "      if (dst <= 2.5*object_width):\n",
        "        regular.append(object_detection)\n",
        "  return regular#, image"
      ],
      "metadata": {
        "id": "z4-nPKZUesZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.spatial import distance\n",
        "\n",
        "# def get_regular_object_fourth(\n",
        "#     person_detections: List[Detection],\n",
        "#     object_detections: List[Detection],\n",
        "#     image\n",
        "# ) -> Optional[Detection]:\n",
        "#   regular=[]\n",
        "#   for object_detection in object_detections:\n",
        "#     for person_detection in person_detections:\n",
        "\n",
        "#       object_center = object_detection.rect.get_center()\n",
        "#       person_center= person_detection.rect.get_center()\n",
        "\n",
        "#       object_width= int(object_detection.rect.width)\n",
        "#       dst = distance.euclidean(object_center, person_center)\n",
        "\n",
        "#       if (dst <= 2.5*object_width):\n",
        "#         regular.append(object_detection)\n",
        "#         image = cv2.line(image,person_center,object_center,(0,255,0),2)\n",
        "#       else:\n",
        "#         image = cv2.line(image,person_center,object_center,(0,0,255),2)\n",
        "#   return regular, image"
      ],
      "metadata": {
        "id": "79YA-Mon1RuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chebyshev distance\n",
        "\n",
        "The method of searching for ordinary objects based on the Chebyshev distance between the centers of the boxes\n",
        "of a person and an object, the distance depends on the width of the box of the object."
      ],
      "metadata": {
        "id": "FMhNlOSQbOPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def get_regular_object_fifth(\n",
        "    person_detections: List[Detection],\n",
        "    object_detections: List[Detection]\n",
        ") -> Optional[Detection]:\n",
        "  regular=[]\n",
        "  for object_detection in object_detections:\n",
        "    for person_detection in person_detections:\n",
        "\n",
        "      object_center = object_detection.rect.get_center()\n",
        "      person_center= person_detection.rect.get_center()\n",
        "      object_width= int(object_detection.rect.width)\n",
        "      dst = distance.chebyshev(object_center, person_center)\n",
        "\n",
        "      if (dst <= 2*object_width):\n",
        "        regular.append(object_detection)\n",
        "       # cv2.line(image,person_center,object_center,(0,0,255),2)\n",
        "\n",
        "  return regular#, image"
      ],
      "metadata": {
        "id": "iKAgNpMS89O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identification of abandoned and suspicious objects"
      ],
      "metadata": {
        "id": "ye5e893HWmEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detections2boxes_for_single_detection(detection):\n",
        "  return np.array([[\n",
        "      detection.rect.top_left.x,\n",
        "      detection.rect.top_left.y,\n",
        "      detection.rect.bottom_right.x,\n",
        "      detection.rect.bottom_right.y\n",
        "      ]], dtype=float)\n",
        "\n",
        "\n",
        "def classification_of_abandoned_objects(arr_of_abandoned_objects,\n",
        "                                        det_of_abandobed_objects, # only one det\n",
        "                                        frame_number):\n",
        "  removable=[]\n",
        "  for j in range(len(det_of_abandobed_objects)):\n",
        "      removable=[]\n",
        "      check_the_same = 0\n",
        "      det_info =  np.empty((1,4), dtype=np.object)\n",
        "      det_info[0][0]= det_of_abandobed_objects[j] # detection\n",
        "      det_info[0][1]= False # statement\n",
        "      det_info[0][2]= [] # ab_track\n",
        "      det_info[0][3]= frame_number # last_frame\n",
        "      det_box = detections2boxes_for_single_detection(det_info[0][0])\n",
        "      arr_of_abandoned_objects=np.append(arr_of_abandoned_objects, det_info, axis=0)\n",
        "\n",
        "      for i in range(len(arr_of_abandoned_objects)):\n",
        "        delta_frame = arr_of_abandoned_objects[i][3]\n",
        "        arr_box = detections2boxes_for_single_detection(arr_of_abandoned_objects[i][0])\n",
        "        if box_iou_batch(arr_box, det_box) > 0.6 and check_the_same!=0:\n",
        "            arr_of_abandoned_objects[i][2].extend(det_info[0][2])\n",
        "            removable.append(i)\n",
        "        if box_iou_batch(arr_box, det_box) > 0.6 and check_the_same==0:\n",
        "            if det_info[0][0].tracker_id != None:\n",
        "              arr_of_abandoned_objects[i][0] = det_info[0][0]\n",
        "            arr_of_abandoned_objects[i][2].append(frame_number)\n",
        "            arr_of_abandoned_objects[i][2].extend(det_info[0][2])\n",
        "            arr_of_abandoned_objects[i][3] = det_info[0][3]\n",
        "            check_the_same = 1\n",
        "      arr_of_abandoned_objects = np.delete(arr_of_abandoned_objects, removable, 0)\n",
        "  arr_of_abandoned_objects = arr_of_abandoned_objects[frame_number - arr_of_abandoned_objects[:, 3] < 200] #10 sec in video\n",
        "  for i in range(len(arr_of_abandoned_objects)):\n",
        "    arr_of_abandoned_objects[i][2] = list((set(arr_of_abandoned_objects[i][2])))\n",
        "    if len(arr_of_abandoned_objects[i][2]) > 100:\n",
        "      ab_t = [x for x in arr_of_abandoned_objects[i][2] if frame_number-x<=200]\n",
        "      if len(ab_t) > 60:\n",
        "        arr_of_abandoned_objects[i][1] = True\n",
        "      ab_t= None\n",
        "\n",
        "  return arr_of_abandoned_objects"
      ],
      "metadata": {
        "id": "6hiebiWTXA6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MOT"
      ],
      "metadata": {
        "id": "ti4vk4UqqA0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_or_empty_file(file_name):\n",
        "    if os.path.exists(file_name):\n",
        "        with open(file_name, 'w') as file:\n",
        "            pass\n",
        "    else:\n",
        "        open(file_name, 'w').close()\n",
        "\"\"\" <frame number>, <object id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <confidence>, <x>, <y>, <z> \"\"\"\n",
        "def mot_txt(filename_for_mot, frame_number, detections):\n",
        "    for det in detections:\n",
        "        string_of_det= str(frame_number)+','+str(det.tracker_id if det.tracker_id!=None else -1)+','\\\n",
        "            +str(round(det.rect.x, 2))+','+str(round(det.rect.y, 2))+','+str(round(det.rect.width, 2))+','\\\n",
        "            +str(round(det.rect.height, 2))+','+str(round(det.confidence, 2))+','+'-1'+','+'-1'+','+'-1'\n",
        "        with open(filename_for_mot, 'r+') as f:\n",
        "            f.seek(0, 2)  # –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞\n",
        "            f.write(f'{string_of_det}\\n')  # —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ, –∑–∞–ø–∏—Å—å"
      ],
      "metadata": {
        "id": "RdFtYzXSp8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Configurations"
      ],
      "metadata": {
        "id": "g8WdLWGomZuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_for_test=2\n",
        "\n",
        "if video_for_test == 1:\n",
        "    dir_of_test=\"/content/TEST/test1\"\n",
        "    video_name=\"test1\"\n",
        "if video_for_test == 2:\n",
        "    dir_of_test=\"/content/TEST/test2\"\n",
        "    video_name=\"test2\"\n",
        "if video_for_test == 3:\n",
        "    dir_of_test=\"/content/TEST/test3\"\n",
        "    video_name=\"test3\"\n",
        "if video_for_test == 4:\n",
        "    dir_of_test=\"/content/TEST/test4\"\n",
        "    video_name=\"test4\"\n",
        "if video_for_test == 5:\n",
        "    dir_of_test=\"/content/TEST/test5\"\n",
        "    video_name=\"test5\"\n"
      ],
      "metadata": {
        "id": "kyGa9nKlnuJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO_PATH = f'{dir_of_test}/video.avi'\n",
        "\n",
        "vid = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "height_of_source = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "width_of_source = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "fps_of_source = int(vid.get(cv2.CAP_PROP_FPS))\n",
        "length_of_source = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "vid.release()\n",
        "\n",
        "el_ratio=(260,100,1630)\n",
        "\n",
        "\n",
        "print(height_of_source)\n",
        "print(width_of_source)\n",
        "print(fps_of_source)\n",
        "print(length_of_source)"
      ],
      "metadata": {
        "id": "ZPxk-R89EuK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e41e74f-aabb-4428-bdb1-c73bb6255ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "414\n",
            "736\n",
            "20\n",
            "977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "lenPBvvWGSs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BYTETracker"
      ],
      "metadata": {
        "id": "Ix4NnTLJAsyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.35\n",
        "    track_buffer: int = 300\n",
        "    match_thresh: float = 0.6\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False\n",
        "\n",
        "\n",
        "# converts List[Detection] into format that can be consumed by match_detections_with_tracks function\n",
        "def detections2boxes_BYTE(detections: List[Detection], with_confidence: bool = True) -> np.ndarray:\n",
        "    return np.array([\n",
        "        [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y,\n",
        "            detection.confidence\n",
        "            # detection.class_id #!!!!!\n",
        "        ] if with_confidence else [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y\n",
        "        ]\n",
        "        for detection\n",
        "        in detections\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
        "def tracks2boxes_BYTE(tracks: List[STrack]) -> np.ndarray:\n",
        "    return np.array([\n",
        "        track.tlbr\n",
        "        for track\n",
        "        in tracks\n",
        "    ], dtype=float)\n",
        "\n",
        "# matches our bounding boxes with predictions\n",
        "def match_detections_with_tracks_BYTE(\n",
        "    detections: List[Detection],\n",
        "    tracks: List[STrack]\n",
        ") -> List[Detection]:\n",
        "    detection_boxes = detections2boxes_BYTE(detections=detections, with_confidence=False)\n",
        "    tracks_boxes = tracks2boxes_BYTE(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detection_boxes)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        detections[detection_index].tracker_id = tracks[tracker_index].track_id\n",
        "\n",
        "    return detections"
      ],
      "metadata": {
        "id": "xLvs2X9dAsGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##StrongSORT"
      ],
      "metadata": {
        "id": "ZyWa2ZsXhj3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from boxmot.trackers.strongsort.strong_sort import StrongSORT\n",
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "\n",
        "def detections2boxes_STRONG(detections: List[Detection], with_confidence: bool = True) -> np.ndarray:\n",
        "    return np.array([\n",
        "        [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y,\n",
        "            detection.confidence,\n",
        "            detection.class_id #!!!!!\n",
        "        ] if with_confidence else [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y\n",
        "        ]\n",
        "        for detection\n",
        "        in detections\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# converts tracks into format that can be consumed by match_detections_with_tracks function\n",
        "def tracks2boxes_STRONG(tracks):\n",
        "    format_tracks = np.empty((0,4))\n",
        "    for i in range(len(tracks)):\n",
        "      arr_2d = np.reshape(tracks[i][:4], (1, 4))\n",
        "      format_tracks=np.append(format_tracks, arr_2d, axis=0)\n",
        "    return format_tracks\n",
        "\n",
        "# matches our bounding boxes with predictions\n",
        "def match_detections_with_tracks_STRONG(\n",
        "    detections: List[Detection],\n",
        "    tracks\n",
        ") -> List[Detection]:\n",
        "    detection_boxes = detections2boxes_STRONG(detections=detections, with_confidence=False)\n",
        "    tracks_boxes = tracks2boxes_STRONG(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detection_boxes)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        track_id=tracks[tracker_index][4].astype('int')\n",
        "        detections[detection_index].tracker_id = track_id#\n",
        "    return detections"
      ],
      "metadata": {
        "id": "lLNu_ChohgsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SORT"
      ],
      "metadata": {
        "id": "xyE5HtOPnjLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "\n",
        "def detections2boxes_SORT(detections: List[Detection], with_confidence: bool = True) -> np.ndarray:\n",
        "    return np.array([\n",
        "        [\n",
        "            detection.rect.min_x,\n",
        "            detection.rect.min_y,\n",
        "            detection.rect.s_width,\n",
        "            detection.rect.s_height,\n",
        "            detection.confidence\n",
        "        ] if with_confidence else [\n",
        "            detection.rect.min_x,\n",
        "            detection.rect.min_y,\n",
        "            detection.rect.s_width,\n",
        "            detection.rect.s_height\n",
        "        ]\n",
        "        for detection\n",
        "        in detections\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# converts tracks into format that can be consumed by match_detections_with_tracks function\n",
        "def tracks2boxes_SORT(tracks):\n",
        "    format_tracks = np.empty((0,4))\n",
        "    for i in range(len(tracks)):\n",
        "      arr_2d = np.reshape(tracks[i][1:5], (1, 4))\n",
        "      format_tracks=np.append(format_tracks, arr_2d, axis=0)\n",
        "    return format_tracks\n",
        "\n",
        "# matches our bounding boxes with predictions\n",
        "def match_detections_with_tracks_SORT(\n",
        "    detections: List[Detection],\n",
        "    tracks\n",
        ") -> List[Detection]:\n",
        "    detection_boxes = detections2boxes_SORT(detections=detections, with_confidence=False)\n",
        "    tracks_boxes = tracks2boxes_SORT(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detection_boxes)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        track_id=tracks[tracker_index][4].astype('int')\n",
        "        detections[detection_index].tracker_id = track_id#\n",
        "    return detections"
      ],
      "metadata": {
        "id": "g1axiGde1pSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##YOLOv5"
      ],
      "metadata": {
        "id": "k-po7yTSrdt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Detections_from_YOLOv5(results):\n",
        "    return results.pred[0].cpu().numpy()"
      ],
      "metadata": {
        "id": "8yAHph28rc_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##YOLOv8"
      ],
      "metadata": {
        "id": "EHpDrCaArlj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Detections_from_YOLOv8(results):\n",
        "    xyxy=results[0].boxes.xyxy.cpu().numpy()\n",
        "    conf=results[0].boxes.conf.cpu().numpy()\n",
        "    class_id=results[0].boxes.cls.cpu().numpy()\n",
        "    return np.column_stack((xyxy,conf,class_id))"
      ],
      "metadata": {
        "id": "dWiRv0Dxrk3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MAIN"
      ],
      "metadata": {
        "id": "H4GdDngVrvjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Annotation Configuration"
      ],
      "metadata": {
        "id": "b93GMqkg_uzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MARKER_CONTOUR_COLOR_HEX = \"#000000\"\n",
        "MARKER_CONTOUR_COLOR = Color.from_hex_string(MARKER_CONTOUR_COLOR_HEX)\n",
        "\n",
        "# SpringGreen\n",
        "REGUAL_OBJECT_FILL_COLOR_HEX = \"#00FF7F\"\n",
        "REGUAL_OBJECT_FILL_COLOR = Color.from_hex_string(REGUAL_OBJECT_FILL_COLOR_HEX)\n",
        "\n",
        "# Dark Red\n",
        "ABANDONED_OBJECT_FILL_COLOR_HEX = \"#8B0000\"\n",
        "ABANDONED_OBJECT_FILL_COLOR = Color.from_hex_string(ABANDONED_OBJECT_FILL_COLOR_HEX)\n",
        "\n",
        "# Blue\n",
        "PERSON_FILL_COLOR_HEX = \"#0000FF\"\n",
        "PERSON_FILL_COLOR = Color.from_hex_string(PERSON_FILL_COLOR_HEX)\n",
        "\n",
        "# Deep Oranje\n",
        "SUSPICIOUS_OBJECT_FILL_COLOR_HEX = \"#C34D0A\"\n",
        "SUSPICIOUS_OBJECT_FILL_COLOR = Color.from_hex_string(SUSPICIOUS_OBJECT_FILL_COLOR_HEX)\n",
        "\n",
        "MARKER_CONTOUR_THICKNESS = 2\n",
        "MARKER_WIDTH = 18\n",
        "MARKER_HEIGHT = 18\n",
        "MARKER_MARGIN = 8\n",
        "\n",
        "person_text_annotator = TextAnnotator(\n",
        "    PERSON_FILL_COLOR, text_color=Color(255, 255, 255), text_thickness=2)\n",
        "\n",
        "regular_object_text_annotator = TextAnnotator(\n",
        "    REGUAL_OBJECT_FILL_COLOR, text_color=Color(0, 0, 0), text_thickness=2)\n",
        "\n",
        "abandoned_object_text_annotator = TextAnnotator(\n",
        "    ABANDONED_OBJECT_FILL_COLOR, text_color=Color(255, 255, 255), text_thickness=2)\n",
        "\n",
        "suspicious_object_text_annotator = TextAnnotator(\n",
        "    SUSPICIOUS_OBJECT_FILL_COLOR, text_color=Color(255, 255, 255), text_thickness=2)\n",
        "\n",
        "\n",
        "regular_object_marker_annotator = MarkerAnntator(\n",
        "    color = REGUAL_OBJECT_FILL_COLOR\n",
        ")\n",
        "\n",
        "abandoned_object_marker_annotator = MarkerAnntator(\n",
        "    color = ABANDONED_OBJECT_FILL_COLOR\n",
        ")\n",
        "\n",
        "suspicious_object_marker_annotator = MarkerAnntator(\n",
        "    color = SUSPICIOUS_OBJECT_FILL_COLOR\n",
        ")\n",
        "\n",
        "person_marker_annotator = MarkerAnntator(\n",
        "    color = PERSON_FILL_COLOR\n",
        ")\n",
        "\n",
        "regular_object_visualization_annotator = ObjectAnnotator(\n",
        "    color = REGUAL_OBJECT_FILL_COLOR\n",
        ")\n",
        "\n",
        "abandoned_object_visualization_annotator = ObjectAnnotator(\n",
        "    color = ABANDONED_OBJECT_FILL_COLOR\n",
        ")\n",
        "\n",
        "suspicious_object_visualization_annotator = ObjectAnnotator(\n",
        "    color = SUSPICIOUS_OBJECT_FILL_COLOR\n",
        ")"
      ],
      "metadata": {
        "id": "NM7rlBcC_p5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ALG"
      ],
      "metadata": {
        "id": "fNH2Nh1rsAYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "id": "QbUf9cn5Aggj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9854d196-c0e6-416d-d3f0-7e2189828735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/Results\n",
        "!rm -rf /content/MOTA"
      ],
      "metadata": {
        "id": "mO4CI8cVAfkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(\"/content/MOTA\"):\n",
        "    os.mkdir(\"/content/MOTA\")\n",
        "\n",
        "for yolo_version in [\"YOLOv5\", \"YOLOv8\"]:\n",
        "    if yolo_version == \"YOLOv5\":\n",
        "        model = torch.hub.load('ultralytics/yolov5', 'custom', f'{dir_of_test}/YOLOv5.pt', device=0)\n",
        "    if yolo_version == \"YOLOv8\":\n",
        "        model = YOLO(f'{dir_of_test}/YOLOv8.pt')\n",
        "        model.fuse()\n",
        "\n",
        "    model.agnostic = False  # NMS class-agnostic\n",
        "    model.multi_label = False  # NMS multiple labels per box\n",
        "    model.max_det = 1000  # maximum number of detections per image\n",
        "    model.names\n",
        "\n",
        "    for tracking_method in ['BYTETracker', 'StrongSORT', 'SORT']:\n",
        "\n",
        "        if tracking_method == \"BYTETracker\":\n",
        "            tracker = BYTETracker(BYTETrackerArgs())\n",
        "        if tracking_method == \"StrongSORT\":\n",
        "            tracker = StrongSORT(\n",
        "                model_weights=Path('osnet_x0_25_msmt17.pt'), # which ReID model to use\n",
        "                device='cuda:0',\n",
        "                max_age=300,\n",
        "                fp16=False,\n",
        "            )\n",
        "        if tracking_method == \"SORT\":\n",
        "            tracker = sort.SORT(max_age=5, min_hits=3, iou_threshold=0.4)\n",
        "\n",
        "        for method_of_ab_ch in ['I', 'II', 'III']:\n",
        "\n",
        "            name_of_dir = \"/content/MOTA/\"+video_name+\"_\"+yolo_version+\"_\"+tracking_method+\"_\"+method_of_ab_ch\n",
        "\n",
        "            if not os.path.isdir(name_of_dir):\n",
        "                os.mkdir(name_of_dir)\n",
        "\n",
        "            for conf_of_yolo in ['CONF=0.3', 'CONF=0.6', 'CONF=0.9']:\n",
        "                if conf_of_yolo == \"CONF=0.3\":\n",
        "                    model.conf = 0.3\n",
        "                if conf_of_yolo == 'CONF=0.6':\n",
        "                    model.conf = 0.6\n",
        "                if conf_of_yolo == 'CONF=0.9':\n",
        "                    model.conf = 0.9\n",
        "\n",
        "                for iou_of_yolo in [\"IOU=0.45\", \"IOU=0.6\", \"IOU=0.75\"]:\n",
        "                    if iou_of_yolo == \"IOU=0.45\":\n",
        "                        model.iou = 0.45\n",
        "                    if iou_of_yolo == \"IOU=0.6\":\n",
        "                        model.iou = 0.6\n",
        "                    if iou_of_yolo == \"IOU=0.75\":\n",
        "                        model.iou = 0.75\n",
        "\n",
        "                    TARGET_VIDEO_PATH = f\"{HOME}/Results/{video_name}_{yolo_version}_{tracking_method}_{method_of_ab_ch}_{conf_of_yolo}_{iou_of_yolo}.mp4\"\n",
        "\n",
        "                    name_of_file_with_dets = name_of_dir+\"/\"+yolo_version+\"_\"+tracking_method+\"_\"+method_of_ab_ch+\"_\"+conf_of_yolo+\"_\"+iou_of_yolo+\".txt\"\n",
        "                    name_of_file_with_abandoned = name_of_dir+\"/\"+yolo_version+\"_\"+tracking_method+\"_\"+method_of_ab_ch+\"_\"+conf_of_yolo+\"_\"+iou_of_yolo+\"_\"+\"ab_ob\"+\".txt\"\n",
        "\n",
        "                    if os.path.exists(name_of_file_with_dets):\n",
        "                        print(\"SKIP\")\n",
        "                        continue\n",
        "                    else:\n",
        "                        create_or_empty_file(name_of_file_with_dets)\n",
        "                        print(name_of_file_with_dets)\n",
        "\n",
        "                    if os.path.exists(name_of_file_with_abandoned):\n",
        "                        print(\"SKIP\")\n",
        "                    else:\n",
        "                        create_or_empty_file(name_of_file_with_abandoned)\n",
        "                        print(name_of_file_with_abandoned)\n",
        "\n",
        "\n",
        "                    # initiate video writer\n",
        "                    video_config = VideoConfig(\n",
        "                        fps=fps_of_source,\n",
        "                        width=width_of_source,\n",
        "                        height=height_of_source)\n",
        "\n",
        "                    video_writer = get_video_writer(\n",
        "                        target_video_path=TARGET_VIDEO_PATH,\n",
        "                        video_config=video_config)\n",
        "                    # get fresh video frame generator\n",
        "                    frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))\n",
        "\n",
        "                    arr_of_abandoned_objects = np.empty((0,4), dtype=np.object)\n",
        "\n",
        "                    ABANDONED_CHECK=0\n",
        "                    # loop over frames\n",
        "                    frame_number=0\n",
        "                    # for frame in frame_iterator:\n",
        "                    for frame in tqdm(frame_iterator, total=length_of_source):\n",
        "                        frame_number=frame_number+1\n",
        "                        # run detector\n",
        "                        results = model(frame)\n",
        "                        if yolo_version==\"YOLOv5\":\n",
        "                            detections = Detection.from_results(\n",
        "                                pred=Detections_from_YOLOv5(results),\n",
        "                                names=model.names)\n",
        "                        if yolo_version==\"YOLOv8\":\n",
        "                            detections = Detection.from_results(\n",
        "                                pred=Detections_from_YOLOv8(results),\n",
        "                                names=model.names)\n",
        "                        if detections != []:\n",
        "                        # Copy of frame for annotations\n",
        "                            annotated_image = frame.copy()\n",
        "                            # run ByteTracker\n",
        "                            if tracking_method==\"BYTETracker\":\n",
        "                                tracks = tracker.update(\n",
        "                                        output_results=detections2boxes_BYTE(detections=detections),\n",
        "                                        img_info=frame.shape,\n",
        "                                        img_size=frame.shape\n",
        "                                    )\n",
        "                            # run StrongSORT\n",
        "                            if tracking_method==\"StrongSORT\":\n",
        "                                if yolo_version==\"YOLOv5\":\n",
        "                                    tracks = tracker.update(Detections_from_YOLOv5(results), frame)\n",
        "                                if yolo_version==\"YOLOv8\":\n",
        "                                    tracks = tracker.update(Detections_from_YOLOv8(results), frame)\n",
        "                            # run SORT\n",
        "                            if tracking_method==\"SORT\":\n",
        "                                tracker.run(detections2boxes_SORT(detections, False), 0)\n",
        "                                tracks = tracker.get_tracks(0)\n",
        "\n",
        "                            if tracks!=[]:\n",
        "                                if tracking_method==\"BYTETracker\":\n",
        "                                    # assigns the ID of tracks to detection\n",
        "                                    tracked_detections = match_detections_with_tracks_BYTE(detections=(detections), tracks=tracks)\n",
        "\n",
        "                                if tracking_method==\"StrongSORT\":\n",
        "                                    # assigns the ID of tracks to detection\n",
        "                                    tracked_detections = match_detections_with_tracks_STRONG(detections=(detections), tracks=tracks)\n",
        "                                if tracking_method==\"SORT\":\n",
        "                                    tracked_detections= match_detections_with_tracks_SORT(detections=(detections), tracks=tracks)\n",
        "\n",
        "                                # Filter detections by class\n",
        "                                tracked_backpack_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"backpack\")\n",
        "                                tracked_handbag_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"handbag\")\n",
        "                                tracked_person_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"person\")\n",
        "                                tracked_suitcase_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"suitcase\")\n",
        "\n",
        "                                tracked_object_detections= tracked_backpack_detections + tracked_handbag_detections + tracked_suitcase_detections\n",
        "                                # calculate regular objects\n",
        "                                if method_of_ab_ch==\"I\":\n",
        "                                    regular_object_detections = get_regular_object(\n",
        "                                        tracked_person_detections,\n",
        "                                        tracked_object_detections\n",
        "                                    )\n",
        "\n",
        "                                if method_of_ab_ch==\"II\":\n",
        "                                    regular_object_detections = get_regular_object_fourth(\n",
        "                                        tracked_person_detections,\n",
        "                                        tracked_object_detections\n",
        "                                    )\n",
        "\n",
        "                                if method_of_ab_ch==\"III\":\n",
        "                                    regular_object_detections = get_regular_object_fifth(\n",
        "                                        tracked_person_detections,\n",
        "                                        tracked_object_detections\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "                                unique_regular_object_detections=[]\n",
        "\n",
        "                                for x in regular_object_detections:\n",
        "                                    if x not in unique_regular_object_detections:\n",
        "                                        unique_regular_object_detections.append(x)\n",
        "\n",
        "                                # calculate abandoned objects\n",
        "                                abandoned_objects_detections = [x for x in tracked_object_detections if x not in unique_regular_object_detections]\n",
        "\n",
        "                                unique_abandoned_objects=[]\n",
        "\n",
        "                                for x in abandoned_objects_detections:\n",
        "                                    if x not in unique_abandoned_objects:\n",
        "                                        unique_abandoned_objects.append(x)\n",
        "\n",
        "\n",
        "                                arr_of_abandoned_objects=classification_of_abandoned_objects(arr_of_abandoned_objects, unique_abandoned_objects, frame_number)\n",
        "\n",
        "                                for_remove = []\n",
        "\n",
        "                                for i in range(len(arr_of_abandoned_objects)):\n",
        "                                    arr_box = detections2boxes_for_single_detection(arr_of_abandoned_objects[i][0])\n",
        "                                    for j in range(len(unique_regular_object_detections)):\n",
        "                                        det_box = detections2boxes_for_single_detection(unique_regular_object_detections[j])\n",
        "                                        if arr_of_abandoned_objects[i][1] == True and  box_iou_batch(arr_box, det_box) > 0.6 and frame_number - arr_of_abandoned_objects[i][3] < 100:\n",
        "                                            for_remove.append(j)\n",
        "\n",
        "                                for_remove = sorted(for_remove, reverse=True)\n",
        "                                for idx in for_remove:\n",
        "                                    if idx < len(unique_regular_object_detections):\n",
        "                                        unique_regular_object_detections.pop(idx)\n",
        "\n",
        "                                detections_MOT=tracked_person_detections+unique_regular_object_detections+unique_abandoned_objects\n",
        "                                mot_txt(name_of_file_with_dets, frame_number, detections_MOT)\n",
        "\n",
        "                                # annotate video frame\n",
        "                                for i in range(len(arr_of_abandoned_objects)):\n",
        "                                    if arr_of_abandoned_objects[i][1] == True and frame_number - arr_of_abandoned_objects[i][3] < 100:\n",
        "                                        ABANDONED_CHECK=1\n",
        "                                        annotated_image = abandoned_object_text_annotator.annotate(\n",
        "                                            image=annotated_image,\n",
        "                                            detections=[arr_of_abandoned_objects[i][0]])\n",
        "                                        annotated_image = abandoned_object_marker_annotator.annotate(\n",
        "                                            image=annotated_image,\n",
        "                                            detections=[arr_of_abandoned_objects[i][0]])\n",
        "\n",
        "                                    if arr_of_abandoned_objects[i][1] == False and frame_number == arr_of_abandoned_objects[i][3]:\n",
        "                                        annotated_image = suspicious_object_text_annotator.annotate(\n",
        "                                            image=annotated_image,\n",
        "                                            detections=[arr_of_abandoned_objects[i][0]])\n",
        "                                        annotated_image = suspicious_object_marker_annotator.annotate(\n",
        "                                            image=annotated_image,\n",
        "                                            detections=[arr_of_abandoned_objects[i][0]])\n",
        "\n",
        "                                if ABANDONED_CHECK == 1:\n",
        "                                    ABANDONED_CHECK=0\n",
        "                                    with open(name_of_file_with_abandoned, 'r+') as f:\n",
        "                                        f.seek(0, 2)  # –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞\n",
        "                                        f.write(f'{frame_number},')  # —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ, –∑–∞–ø–∏—Å—å\n",
        "\n",
        "                                annotated_image = person_text_annotator.annotate(\n",
        "                                    image=annotated_image,\n",
        "                                    detections=tracked_person_detections)\n",
        "\n",
        "                                annotated_image = regular_object_text_annotator.annotate(\n",
        "                                    image=annotated_image,\n",
        "                                    detections=unique_regular_object_detections)\n",
        "\n",
        "\n",
        "                                annotated_image = person_marker_annotator.annotate(\n",
        "                                    image=annotated_image,\n",
        "                                    detections=tracked_person_detections)\n",
        "\n",
        "                                annotated_image = regular_object_marker_annotator.annotate(\n",
        "                                    image=annotated_image,\n",
        "                                    detections=unique_regular_object_detections)\n",
        "\n",
        "                            # save video frame\n",
        "                            video_writer.write(annotated_image)\n",
        "\n",
        "                        else:\n",
        "                            video_writer.write(frame)\n",
        "\n",
        "                    video_writer.release()"
      ],
      "metadata": {
        "id": "ZUhBw0eGr_xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r MOTA1.zip /content/MOTA"
      ],
      "metadata": {
        "id": "OT2Wy9OiEWwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp MOTA1.zip \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "IPUtJ0nsETLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END"
      ],
      "metadata": {
        "id": "xWbkb50IWwGx"
      }
    }
  ]
}